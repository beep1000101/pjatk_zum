{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR Commands (mini_speech_commands) â€“ training + evaluation notebook\n",
    "# Mirrors the end-to-end style of sentiment_embeddings/colab_training.ipynb.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"data_ingestion\").exists() and (p / \"utils\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from utils.paths import CACHE_PATH  # noqa: E402\n",
    "\n",
    "CACHE = CACHE_PATH\n",
    "PIPELINE_CACHE = CACHE / \"asr_commands\"\n",
    "RAW_DIR = PIPELINE_CACHE / \"raw\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\" / \"asr_commands\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9596aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dataset is present in cache (runs ingestion if needed)\n",
    "mini_root = RAW_DIR / \"mini_speech_commands\"\n",
    "if not mini_root.exists():\n",
    "    print(\"mini_speech_commands not found in cache; running ingestion...\")\n",
    "    !python data_ingestion/asr_commands/run.py\n",
    "\n",
    "assert mini_root.exists(), f\"Missing: {mini_root}\"\n",
    "\n",
    "labels_path = PIPELINE_CACHE / \"labels.json\"\n",
    "if labels_path.exists():\n",
    "    labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "else:\n",
    "    # Fallback: infer from directories\n",
    "    labels = sorted([p.name for p in mini_root.iterdir() if p.is_dir()])\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d263554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect WAV files and build deterministic train/val/test splits (stratified per label)\n",
    "all_rows = []\n",
    "for lbl in labels:\n",
    "    wavs = sorted((mini_root / lbl).glob(\"*.wav\"))\n",
    "    for p in wavs:\n",
    "        all_rows.append({\"path\": str(p), \"label\": lbl})\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic split: per label 80/10/10 (train/val/test)\n",
    "rng = np.random.default_rng(42)\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for lbl in labels:\n",
    "    sub = df[df[\"label\"] == lbl].copy()\n",
    "    idx = sub.index.to_numpy()\n",
    "    rng.shuffle(idx)\n",
    "    n = len(idx)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train : n_train + n_val]\n",
    "    test_idx = idx[n_train + n_val :]\n",
    "    splits[\"train\"].extend(df.loc[train_idx].to_dict(orient=\"records\"))\n",
    "    splits[\"val\"].extend(df.loc[val_idx].to_dict(orient=\"records\"))\n",
    "    splits[\"test\"].extend(df.loc[test_idx].to_dict(orient=\"records\"))\n",
    "\n",
    "{k: len(v) for k, v in splits.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) persist splits for reuse (matches project pipeline idea)\n",
    "(OUTPUTS_DIR / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "splits_path = OUTPUTS_DIR / \"preprocessing\" / \"splits.json\"\n",
    "splits_payload = {\"labels\": labels, \"splits\": splits}\n",
    "splits_path.write_text(json.dumps(splits_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "splits_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a31c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio feature pipeline (torchaudio log-mel) + Dataset\n",
    "try:\n",
    "    import torchaudio\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"torchaudio is required for this notebook. Install it in your env (uv/pip) and restart kernel.\\n\"\n",
    "        f\"Original import error: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "label_to_id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id_to_label = {i: lbl for lbl, i in label_to_id.items()}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Example:\n",
    "    path: Path\n",
    "    label_id: int\n",
    "\n",
    "\n",
    "def make_examples(items: list[dict]):\n",
    "    out = []\n",
    "    for it in items:\n",
    "        out.append(Example(path=Path(it[\"path\"]), label_id=label_to_id[it[\"label\"]]))\n",
    "    return out\n",
    "\n",
    "\n",
    "train_examples = make_examples(splits[\"train\"])\n",
    "val_examples = make_examples(splits[\"val\"])\n",
    "test_examples = make_examples(splits[\"test\"])\n",
    "\n",
    "len(train_examples), len(val_examples), len(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a877820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *, examples: list[Example], sample_rate: int = 16000, clip_seconds: float = 1.0, n_mels: int = 64):\n",
    "        self.examples = examples\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = int(sample_rate * clip_seconds)\n",
    "\n",
    "        self._resamplers: dict[int, torchaudio.transforms.Resample] = {}\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            n_mels=n_mels,\n",
    "        )\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def _resample(self, wav: torch.Tensor, orig_sr: int) -> torch.Tensor:\n",
    "        if orig_sr == self.sample_rate:\n",
    "            return wav\n",
    "        if orig_sr not in self._resamplers:\n",
    "            self._resamplers[orig_sr] = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=self.sample_rate)\n",
    "        return self._resamplers[orig_sr](wav)\n",
    "\n",
    "    def _pad_or_trim(self, wav: torch.Tensor) -> torch.Tensor:\n",
    "        # wav: (1, T)\n",
    "        T = int(wav.shape[-1])\n",
    "        if T == self.num_samples:\n",
    "            return wav\n",
    "        if T > self.num_samples:\n",
    "            return wav[..., : self.num_samples]\n",
    "        pad = self.num_samples - T\n",
    "        return torch.nn.functional.pad(wav, (0, pad))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ex = self.examples[idx]\n",
    "        wav, sr = torchaudio.load(ex.path)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav = self._resample(wav, sr)\n",
    "        wav = self._pad_or_trim(wav)\n",
    "\n",
    "        spec = self.melspec(wav)          # (1, n_mels, time)\n",
    "        spec = self.to_db(spec)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        return spec, ex.label_id\n",
    "\n",
    "\n",
    "train_ds = SpeechCommandsDataset(examples=train_examples)\n",
    "val_ds = SpeechCommandsDataset(examples=val_examples)\n",
    "test_ds = SpeechCommandsDataset(examples=test_examples)\n",
    "\n",
    "x0, y0 = train_ds[0]\n",
    "x0.shape, y0, id_to_label[y0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN baseline (like a compact \"training stage\")\n",
    "class KwsCnn(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    x = torch.stack([b[0] for b in batch], dim=0)\n",
    "    y = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate)\n",
    "\n",
    "model = KwsCnn(n_classes=len(labels)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (tracks best validation accuracy)\n",
    "def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    return float((preds == y).float().mean().item())\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "best_val_acc = -math.inf\n",
    "history = []\n",
    "\n",
    "best_dir = OUTPUTS_DIR / \"best\"\n",
    "best_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = best_dir / \"kws_cnn.pt\"\n",
    "labels_out_path = best_dir / \"labels.json\"\n",
    "labels_out_path.write_text(json.dumps({\"labels\": labels}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_losses, train_accs = [], []\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(float(loss.item()))\n",
    "        train_accs.append(accuracy_from_logits(logits.detach(), y))\n",
    "\n",
    "    model.eval()\n",
    "    val_losses, val_accs = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            val_losses.append(float(loss.item()))\n",
    "            val_accs.append(accuracy_from_logits(logits, y))\n",
    "\n",
    "    rec = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(np.mean(train_losses)),\n",
    "        \"train_acc\": float(np.mean(train_accs)),\n",
    "        \"val_loss\": float(np.mean(val_losses)),\n",
    "        \"val_acc\": float(np.mean(val_accs)),\n",
    "    }\n",
    "    history.append(rec)\n",
    "    print(rec)\n",
    "\n",
    "    if rec[\"val_acc\"] > best_val_acc:\n",
    "        best_val_acc = rec[\"val_acc\"]\n",
    "        torch.save(\n",
    "            {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"labels\": labels,\n",
    "                \"sample_rate\": 16000,\n",
    "                \"clip_seconds\": 1.0,\n",
    "                \"n_mels\": 64,\n",
    "            },\n",
    "            ckpt_path,\n",
    "        )\n",
    "\n",
    "train_metrics_path = OUTPUTS_DIR / \"training\" / \"metrics.json\"\n",
    "train_metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "train_metrics_path.write_text(json.dumps({\"pipeline\": \"asr_commands\", \"history\": history}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "ckpt_path, train_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd396e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint and evaluate on the test split\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += int((preds == y).sum().item())\n",
    "        total += int(y.numel())\n",
    "        y_true.extend([int(v) for v in y.detach().cpu().tolist()])\n",
    "        y_pred.extend([int(v) for v in preds.detach().cpu().tolist()])\n",
    "\n",
    "test_acc = correct / total if total else 0.0\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + metrics export\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[int(t), int(p)] += 1\n",
    "\n",
    "eval_metrics = {\n",
    "    \"pipeline\": \"asr_commands\",\n",
    "    \"num_test\": int(total),\n",
    "    \"accuracy\": float(test_acc),\n",
    "    \"labels\": labels,\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "}\n",
    "\n",
    "eval_dir = OUTPUTS_DIR / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_metrics_path = eval_dir / \"metrics.json\"\n",
    "eval_metrics_path.write_text(json.dumps(eval_metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "eval_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Zip outputs for sharing (e.g., Colab -> download)\n",
    "!zip -r asr_commands_outputs.zip outputs/asr_commands"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

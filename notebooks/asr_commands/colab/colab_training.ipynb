{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e858155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"data_ingestion\").exists() and (p / \"utils\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from utils.paths import CACHE_PATH  # noqa: E402\n",
    "\n",
    "CACHE = CACHE_PATH\n",
    "PIPELINE_CACHE = CACHE / \"asr_commands\"\n",
    "RAW_DIR = PIPELINE_CACHE / \"raw\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\" / \"asr_commands\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9596aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure dataset is present in cache (runs ingestion if needed)\n",
    "mini_root = RAW_DIR / \"mini_speech_commands\"\n",
    "if not mini_root.exists():\n",
    "    print(\"mini_speech_commands not found in cache; running ingestion...\")\n",
    "    !python data_ingestion/asr_commands/run.py\n",
    "\n",
    "assert mini_root.exists(), f\"Missing: {mini_root}\"\n",
    "\n",
    "labels_path = PIPELINE_CACHE / \"labels.json\"\n",
    "if labels_path.exists():\n",
    "    labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "else:\n",
    "    # Fallback: infer from directories\n",
    "    labels = sorted([p.name for p in mini_root.iterdir() if p.is_dir()])\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d263554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "down     1000\n",
       "go       1000\n",
       "left     1000\n",
       "no       1000\n",
       "right    1000\n",
       "stop     1000\n",
       "up       1000\n",
       "yes      1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect WAV files and build deterministic train/val/test splits (stratified per label)\n",
    "all_rows = []\n",
    "for lbl in labels:\n",
    "    wavs = sorted((mini_root / lbl).glob(\"*.wav\"))\n",
    "    for p in wavs:\n",
    "        all_rows.append({\"path\": str(p), \"label\": lbl})\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a08e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6400, 'val': 800, 'test': 800}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deterministic split: per label 80/10/10 (train/val/test)\n",
    "rng = np.random.default_rng(42)\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for lbl in labels:\n",
    "    sub = df[df[\"label\"] == lbl].copy()\n",
    "    idx = sub.index.to_numpy()\n",
    "    # rng.shuffle(idx)\n",
    "    n = len(idx)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train : n_train + n_val]\n",
    "    test_idx = idx[n_train + n_val :]\n",
    "    splits[\"train\"].extend(df.loc[train_idx].to_dict(orient=\"records\"))\n",
    "    splits[\"val\"].extend(df.loc[val_idx].to_dict(orient=\"records\"))\n",
    "    splits[\"test\"].extend(df.loc[test_idx].to_dict(orient=\"records\"))\n",
    "\n",
    "{k: len(v) for k, v in splits.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0eda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mateusz/dev/pjatk_zum/outputs/asr_commands/preprocessing/splits.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) persist splits for reuse (matches project pipeline idea)\n",
    "(OUTPUTS_DIR / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "splits_path = OUTPUTS_DIR / \"preprocessing\" / \"splits.json\"\n",
    "splits_payload = {\"labels\": labels, \"splits\": splits}\n",
    "splits_path.write_text(json.dumps(splits_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "splits_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a31c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 800, 800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audio feature pipeline (torchaudio log-mel) + Dataset\n",
    "try:\n",
    "    import torchaudio\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"torchaudio is required for this notebook. Install it in your env (uv/pip) and restart kernel.\\n\"\n",
    "        f\"Original import error: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "label_to_id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id_to_label = {i: lbl for lbl, i in label_to_id.items()}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Example:\n",
    "    path: Path\n",
    "    label_id: int\n",
    "\n",
    "\n",
    "def make_examples(items: list[dict]):\n",
    "    out = []\n",
    "    for it in items:\n",
    "        out.append(Example(path=Path(it[\"path\"]), label_id=label_to_id[it[\"label\"]]))\n",
    "    return out\n",
    "\n",
    "\n",
    "train_examples = make_examples(splits[\"train\"])\n",
    "val_examples = make_examples(splits[\"val\"])\n",
    "test_examples = make_examples(splits[\"test\"])\n",
    "\n",
    "len(train_examples), len(val_examples), len(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a877820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m val_ds = SpeechCommandsDataset(examples=val_examples)\n\u001b[32m     52\u001b[39m test_ds = SpeechCommandsDataset(examples=test_examples)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x0, y0 = \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m x0.shape, y0, id_to_label[y0]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mSpeechCommandsDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     37\u001b[39m     ex = \u001b[38;5;28mself\u001b[39m.examples[idx]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     wav, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wav.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m:\n\u001b[32m     40\u001b[39m         wav = wav.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:84\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Parameter validation and warnings\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m normalize:\n",
      "\u001b[31mImportError\u001b[39m: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function."
     ]
    }
   ],
   "source": [
    "class SpeechCommandsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *, examples: list[Example], sample_rate: int = 16000, clip_seconds: float = 1.0, n_mels: int = 64):\n",
    "        self.examples = examples\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = int(sample_rate * clip_seconds)\n",
    "\n",
    "        self._resamplers: dict[int, torchaudio.transforms.Resample] = {}\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            n_mels=n_mels,\n",
    "        )\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def _resample(self, wav: torch.Tensor, orig_sr: int) -> torch.Tensor:\n",
    "        if orig_sr == self.sample_rate:\n",
    "            return wav\n",
    "        if orig_sr not in self._resamplers:\n",
    "            self._resamplers[orig_sr] = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=self.sample_rate)\n",
    "        return self._resamplers[orig_sr](wav)\n",
    "\n",
    "    def _pad_or_trim(self, wav: torch.Tensor) -> torch.Tensor:\n",
    "        # wav: (1, T)\n",
    "        T = int(wav.shape[-1])\n",
    "        if T == self.num_samples:\n",
    "            return wav\n",
    "        if T > self.num_samples:\n",
    "            return wav[..., : self.num_samples]\n",
    "        pad = self.num_samples - T\n",
    "        return torch.nn.functional.pad(wav, (0, pad))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ex = self.examples[idx]\n",
    "        wav, sr = torchaudio.load(ex.path)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav = self._resample(wav, sr)\n",
    "        wav = self._pad_or_trim(wav)\n",
    "\n",
    "        spec = self.melspec(wav)          # (1, n_mels, time)\n",
    "        spec = self.to_db(spec)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        return spec, ex.label_id\n",
    "\n",
    "\n",
    "train_ds = SpeechCommandsDataset(examples=train_examples)\n",
    "val_ds = SpeechCommandsDataset(examples=val_examples)\n",
    "test_ds = SpeechCommandsDataset(examples=test_examples)\n",
    "\n",
    "x0, y0 = train_ds[0]\n",
    "x0.shape, y0, id_to_label[y0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN baseline (like a compact \"training stage\")\n",
    "class KwsCnn(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    x = torch.stack([b[0] for b in batch], dim=0)\n",
    "    y = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate)\n",
    "\n",
    "model = KwsCnn(n_classes=len(labels)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (tracks best validation accuracy)\n",
    "def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    return float((preds == y).float().mean().item())\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "best_val_acc = -math.inf\n",
    "history = []\n",
    "\n",
    "best_dir = OUTPUTS_DIR / \"best\"\n",
    "best_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = best_dir / \"kws_cnn.pt\"\n",
    "labels_out_path = best_dir / \"labels.json\"\n",
    "labels_out_path.write_text(json.dumps({\"labels\": labels}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_losses, train_accs = [], []\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(float(loss.item()))\n",
    "        train_accs.append(accuracy_from_logits(logits.detach(), y))\n",
    "\n",
    "    model.eval()\n",
    "    val_losses, val_accs = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            val_losses.append(float(loss.item()))\n",
    "            val_accs.append(accuracy_from_logits(logits, y))\n",
    "\n",
    "    rec = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(np.mean(train_losses)),\n",
    "        \"train_acc\": float(np.mean(train_accs)),\n",
    "        \"val_loss\": float(np.mean(val_losses)),\n",
    "        \"val_acc\": float(np.mean(val_accs)),\n",
    "    }\n",
    "    history.append(rec)\n",
    "    print(rec)\n",
    "\n",
    "    if rec[\"val_acc\"] > best_val_acc:\n",
    "        best_val_acc = rec[\"val_acc\"]\n",
    "        torch.save(\n",
    "            {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"labels\": labels,\n",
    "                \"sample_rate\": 16000,\n",
    "                \"clip_seconds\": 1.0,\n",
    "                \"n_mels\": 64,\n",
    "            },\n",
    "            ckpt_path,\n",
    "        )\n",
    "\n",
    "train_metrics_path = OUTPUTS_DIR / \"training\" / \"metrics.json\"\n",
    "train_metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "train_metrics_path.write_text(json.dumps({\"pipeline\": \"asr_commands\", \"history\": history}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "ckpt_path, train_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd396e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint and evaluate on the test split\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += int((preds == y).sum().item())\n",
    "        total += int(y.numel())\n",
    "        y_true.extend([int(v) for v in y.detach().cpu().tolist()])\n",
    "        y_pred.extend([int(v) for v in preds.detach().cpu().tolist()])\n",
    "\n",
    "test_acc = correct / total if total else 0.0\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + metrics export\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[int(t), int(p)] += 1\n",
    "\n",
    "eval_metrics = {\n",
    "    \"pipeline\": \"asr_commands\",\n",
    "    \"num_test\": int(total),\n",
    "    \"accuracy\": float(test_acc),\n",
    "    \"labels\": labels,\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "}\n",
    "\n",
    "eval_dir = OUTPUTS_DIR / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_metrics_path = eval_dir / \"metrics.json\"\n",
    "eval_metrics_path.write_text(json.dumps(eval_metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "eval_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Zip outputs for sharing (e.g., Colab -> download)\n",
    "!zip -r asr_commands_outputs.zip outputs/asr_commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

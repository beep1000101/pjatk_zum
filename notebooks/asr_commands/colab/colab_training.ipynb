{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df246a9",
   "metadata": {},
   "source": [
    "# ASR Commands (Keyword Spotting) — Hugging Face Baseline\n",
    "\n",
    "This notebook implements a complete deep learning pipeline for keyword spotting (speech command classification) using TensorFlow's mini_speech_commands dataset. It is the reference implementation for the `asr_commands` pipeline in this repository.\n",
    "\n",
    "**Purpose:**\n",
    "- Train a model to classify short audio clips into spoken command words.\n",
    "- Demonstrate data ingestion, preprocessing, model training, evaluation, and artifact export using Hugging Face Transformers and the mini_speech_commands dataset.\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. **Repository Setup & Data Ingestion:**\n",
    "   - Ensures the notebook is running in the correct project directory.\n",
    "   - Downloads and extracts the mini_speech_commands dataset into `.cache/asr_commands/` using the provided ingestion script.\n",
    "2. **Environment Preparation:**\n",
    "   - Installs required Python packages and system dependencies for audio processing and model training.\n",
    "3. **Data Loading & Preprocessing:**\n",
    "   - Loads audio files and labels from the dataset.\n",
    "   - Splits data into train/validation/test sets.\n",
    "   - Prepares data for Hugging Face audio classification models.\n",
    "4. **Model Training:**\n",
    "   - Fine-tunes a pretrained audio classification model (e.g., HuBERT) using the Hugging Face `Trainer` API.\n",
    "   - Saves model checkpoints and training logs.\n",
    "5. **Evaluation & Export:**\n",
    "   - Evaluates the model on the test set.\n",
    "   - Computes accuracy, macro F1, and confusion matrix.\n",
    "   - Saves metrics and artifacts to the `outputs/asr_commands/` directory.\n",
    "\n",
    "**Inputs:**\n",
    "- mini_speech_commands dataset (downloaded to `.cache/asr_commands/raw/mini_speech_commands/`).\n",
    "- Configuration and utility code from the repository.\n",
    "\n",
    "**Outputs:**\n",
    "- Trained model checkpoints (Hugging Face format).\n",
    "- Evaluation metrics (JSON, confusion matrix).\n",
    "- Optional zipped artifacts for download.\n",
    "\n",
    "**Assumptions:**\n",
    "- The notebook is run in a Colab or local environment with internet access.\n",
    "- All code and data paths are relative to the repository root.\n",
    "\n",
    "**Relevant Files & Directories:**\n",
    "- `data_ingestion/asr_commands/run.py`: Ingestion script for downloading the dataset.\n",
    "- `.cache/asr_commands/`: Directory for cached raw data.\n",
    "- `outputs/asr_commands/`: Directory for model outputs and metrics.\n",
    "- `utils/paths.py`: Utility for resolving cache paths.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13890af",
   "metadata": {},
   "source": [
    "# ASR Commands (Keyword Spotting) — Production-grade HF baseline\n",
    "\n",
    "This notebook trains a **keyword spotting / speech command classification** model on TensorFlow's **mini_speech_commands** dataset.\n",
    "\n",
    "## Design goals\n",
    "- Minimal custom modeling code (rely on mature ecosystem).\n",
    "- Deterministic splits + reproducibility.\n",
    "- Clean artifact export (model + processor + labels + metrics).\n",
    "\n",
    "## Architecture\n",
    "1) `data_ingestion/asr_commands` downloads+verifies dataset into `.cache/`\n",
    "2) Dataset is loaded as `{audio, label}` with `datasets.Audio(sampling_rate=16_000)`\n",
    "3) Model is fine-tuned via `transformers.Trainer` using a pretrained speech encoder (`HuBERT` / `Wav2Vec2`)\n",
    "4) Evaluation exports accuracy, macro-F1, and confusion matrix into `outputs/asr_commands/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a404ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL RESET\n",
    "!pip uninstall -y torch torchvision torchaudio torchcodec transformers datasets evaluate accelerate scikit-learn soundfile fastai peft sentence-transformers timm\n",
    "\n",
    "# Install PyTorch stack (CUDA 12.1, stable)\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \\\n",
    "  --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# System deps\n",
    "!apt-get update -qq\n",
    "!apt-get install -y ffmpeg libsndfile1\n",
    "\n",
    "# Install compatible HF stack (PINNED)\n",
    "%pip install \\\n",
    "  torchcodec==0.2.0 \\\n",
    "  transformers==4.41.2 \\\n",
    "  datasets==2.19.1 \\\n",
    "  evaluate==0.4.2 \\\n",
    "  accelerate==0.29.3 \\\n",
    "  scikit-learn==1.4.2 \\\n",
    "  soundfile==0.12.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac4359",
   "metadata": {},
   "source": [
    "## 1. Environment Preparation\n",
    "This section installs and configures all required Python packages and system dependencies for audio processing and model training. It ensures compatibility with CUDA (if available) and installs the correct versions of PyTorch, Hugging Face Transformers, and other libraries.\n",
    "\n",
    "- **Inputs:** None (installs packages as needed).\n",
    "- **Outputs:** Required packages and system dependencies available in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc800dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f90729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab convenience: clone repo if needed\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"pyproject.toml\").exists():\n",
    "    if not Path(\"pjatk_zum\").exists():\n",
    "        !git clone https://github.com/beep1000101/pjatk_zum.git\n",
    "    else:\n",
    "        print(\"Repo folder already present: pjatk_zum\")\n",
    "else:\n",
    "    print(\"Already in repo root (pyproject.toml found)\")\n",
    "\n",
    "# Colab convenience: cd into repo folder if we cloned it\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"pyproject.toml\").exists():\n",
    "    print(\"Already in repo root\")\n",
    "elif Path(\"pjatk_zum\").exists():\n",
    "    %cd pjatk_zum\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find repo root (pyproject.toml) or ./pjatk_zum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f460063",
   "metadata": {},
   "source": [
    "## 2. Repository Setup & Data Ingestion\n",
    "This section ensures the notebook is running in the correct project directory. If the repository is not present, it is cloned from GitHub. The mini_speech_commands dataset is then downloaded and extracted into `.cache/asr_commands/` using the ingestion script (`data_ingestion/asr_commands/run.py`).\n",
    "\n",
    "- **Inputs:** None (repository and dataset are fetched if missing).\n",
    "- **Outputs:** `.cache/asr_commands/raw/mini_speech_commands/` directory with extracted audio data.\n",
    "- **Assumptions:** Internet access is available for cloning and downloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest mini_speech_commands into the repo cache\n",
    "!python data_ingestion/asr_commands/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e858155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import Audio, Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForAudioClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    " )\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"data_ingestion\").exists() and (p / \"utils\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from utils.paths import CACHE_PATH  # noqa: E402\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "CACHE = CACHE_PATH\n",
    "PIPELINE_CACHE = CACHE / \"asr_commands\"\n",
    "RAW_DIR = PIPELINE_CACHE / \"raw\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\" / \"asr_commands\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a448bab",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing\n",
    "This section loads the audio files and their labels from the extracted dataset, builds a DataFrame, and splits the data into training, validation, and test sets. The splits are stratified and deterministic for reproducibility. The data is then prepared for use with Hugging Face audio classification models.\n",
    "\n",
    "- **Inputs:** `.cache/asr_commands/raw/mini_speech_commands/` directory containing WAV files organized by label.\n",
    "- **Outputs:** Train/validation/test splits and Hugging Face `DatasetDict` objects.\n",
    "- **Assumptions:** The dataset is present in the expected cache location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9596aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure dataset is present in cache (runs ingestion if needed)\n",
    "mini_root = RAW_DIR / \"mini_speech_commands\"\n",
    "if not mini_root.exists():\n",
    "    print(\"mini_speech_commands not found in cache; running ingestion...\")\n",
    "    !python data_ingestion/asr_commands/run.py\n",
    "\n",
    "assert mini_root.exists(), f\"Missing: {mini_root}\"\n",
    "\n",
    "labels_path = PIPELINE_CACHE / \"labels.json\"\n",
    "if labels_path.exists():\n",
    "    labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "else:\n",
    "    # Fallback: infer from directories\n",
    "    labels = sorted([p.name for p in mini_root.iterdir() if p.is_dir()])\n",
    "\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "labels, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d263554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "down     1000\n",
       "go       1000\n",
       "left     1000\n",
       "no       1000\n",
       "right    1000\n",
       "stop     1000\n",
       "up       1000\n",
       "yes      1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an index of audio files -> labels\n",
    "rows = []\n",
    "for lbl in labels:\n",
    "    for wav_path in sorted((mini_root / lbl).glob(\"*.wav\")):\n",
    "        rows.append({\"audio\": str(wav_path), \"label_str\": lbl, \"label\": int(label2id[lbl])})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"label_str\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a08e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6400, 'val': 800, 'test': 800}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deterministic stratified split 80/10/10\n",
    "# NOTE: We use sklearn here to avoid known NumPy 2.x incompatibilities in some `datasets` split paths.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"],\n",
    " )\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=temp_df[\"label\"],\n",
    " )\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True), preserve_index=False),\n",
    "    \"val\": Dataset.from_pandas(val_df.reset_index(drop=True), preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True), preserve_index=False),\n",
    "})\n",
    "\n",
    "{k: len(v) for k, v in ds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0eda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mateusz/dev/pjatk_zum/outputs/asr_commands/preprocessing/splits.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist splits for reuse (stable artifact for later stage-first scripts)\n",
    "(OUTPUTS_DIR / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "splits_path = OUTPUTS_DIR / \"preprocessing\" / \"splits.json\"\n",
    "\n",
    "def _to_records(dset: Dataset) -> list[dict]:\n",
    "    # Keep the same schema used elsewhere in the repo\n",
    "    return [{\"path\": ex[\"audio\"], \"label\": id2label[int(ex[\"label\"]) ]} for ex in dset]\n",
    "\n",
    "payload = {\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"splits\": {\n",
    "        \"train\": _to_records(ds[\"train\"]),\n",
    "        \"val\": _to_records(ds[\"val\"]),\n",
    "        \"test\": _to_records(ds[\"test\"]),\n",
    "    },\n",
    "}\n",
    "splits_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "splits_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a31c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 800, 800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HF audio pipeline: decode/resample -> feature_extractor -> padded batches\n",
    "# Model choice: HuBERT encoder pretrained + KWS-oriented finetuning (SUPERB)\n",
    "MODEL_NAME = \"superb/hubert-base-superb-ks\"\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Decode audio lazily; `datasets` uses soundfile under the hood\n",
    "ds_audio = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    out = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = out[\"input_values\"][0]\n",
    "    if \"attention_mask\" in out:\n",
    "        batch[\"attention_mask\"] = out[\"attention_mask\"][0]\n",
    "    return batch\n",
    "\n",
    "ds_proc = ds_audio.map(preprocess_batch, remove_columns=[\"audio\"])\n",
    "\n",
    "# transformers expects `tokenizer=` here (works for feature extractors too)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=feature_extractor, padding=True)\n",
    "\n",
    "ds_proc[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a85c15",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "This section fine-tunes a pretrained audio classification model (e.g., HuBERT) using the Hugging Face `Trainer` API. The model is trained on the processed training set and evaluated on the validation set. Training arguments, such as batch size, learning rate, and number of epochs, are specified here.\n",
    "\n",
    "- **Inputs:** Processed training and validation datasets.\n",
    "- **Outputs:** Trained model checkpoints and training logs saved to `outputs/asr_commands/`.\n",
    "- **Assumptions:** Sufficient compute resources are available for training (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a877820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m val_ds = SpeechCommandsDataset(examples=val_examples)\n\u001b[32m     52\u001b[39m test_ds = SpeechCommandsDataset(examples=test_examples)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x0, y0 = \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m x0.shape, y0, id_to_label[y0]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mSpeechCommandsDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     37\u001b[39m     ex = \u001b[38;5;28mself\u001b[39m.examples[idx]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     wav, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wav.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m:\n\u001b[32m     40\u001b[39m         wav = wav.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:84\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Parameter validation and warnings\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m normalize:\n",
      "\u001b[31mImportError\u001b[39m: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function."
     ]
    }
   ],
   "source": [
    "# Trainer setup + fine-tuning\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Correctly access predictions and label_ids from the EvalPrediction object\n",
    "    logits = eval_pred.predictions\n",
    "    labels_np = eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels_np)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels_np, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": float(acc), \"f1_macro\": float(f1)}\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Stability on small datasets: freeze encoder features (optional but recommended)\n",
    "if hasattr(model, \"freeze_feature_encoder\"):\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "run_dir = OUTPUTS_DIR / \"hf_training\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(run_dir),\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    " )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_proc[\"train\"],\n",
    "    eval_dataset=ds_proc[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model + feature extractor + labels (portable inference package)\n",
    "best_dir = OUTPUTS_DIR / \"best_hf\"\n",
    "best_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(best_dir))\n",
    "feature_extractor.save_pretrained(str(best_dir))\n",
    "(best_dir / \"labels.json\").write_text(json.dumps({\"labels\": labels, \"label2id\": label2id}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "best_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training log history (JSON)\n",
    "train_metrics_path = OUTPUTS_DIR / \"training\" / \"metrics.json\"\n",
    "train_metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history = trainer.state.log_history\n",
    "train_metrics_path.write_text(\n",
    "    json.dumps({\"pipeline\": \"asr_commands\", \"framework\": \"hf_trainer\", \"history\": history}, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "train_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd396e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test split\n",
    "test_metrics = trainer.evaluate(ds_proc[\"test\"])\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29146ad",
   "metadata": {},
   "source": [
    "### Validation Evaluation\n",
    "This cell evaluates the trained model on the validation or test set and reports the accuracy and macro F1 score. These metrics are used to assess model performance and select the best checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + full metrics export\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "pred_out = trainer.predict(ds_proc[\"test\"])\n",
    "logits = pred_out.predictions\n",
    "y_true = pred_out.label_ids\n",
    "y_pred = np.argmax(logits[0], axis=-1) # Access the first element of the tuple for logits\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=list(range(len(labels))),\n",
    "    target_names=labels,\n",
    "    output_dict=True,\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "eval_metrics = {\n",
    "    \"pipeline\": \"asr_commands\",\n",
    "    \"num_test\": int(len(y_true)),\n",
    "    \"accuracy\": float(test_metrics.get(\"eval_accuracy\", test_metrics.get(\"accuracy\", 0.0))),\n",
    "    \"f1_macro\": float(test_metrics.get(\"eval_f1_macro\", test_metrics.get(\"f1_macro\", 0.0))),\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"classification_report\": report,\n",
    "}\n",
    "\n",
    "eval_dir = OUTPUTS_DIR / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_metrics_path = eval_dir / \"metrics.json\"\n",
    "eval_metrics_path.write_text(json.dumps(eval_metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "eval_metrics_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb881574",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Metrics Export\n",
    "This cell computes the confusion matrix and a detailed classification report for the test set. The results, along with accuracy and macro F1, are saved to a JSON file in the evaluation directory for reproducibility and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Zip outputs for sharing (e.g., Colab -> download)\n",
    "!zip -r asr_commands_outputs.zip outputs/asr_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation: Run after training ---\n",
    "from helpers import run_inference, compute_metrics, load_test_dataset\n",
    "import json\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Load best model and feature extractor\n",
    "best_dir = Path(OUTPUTS_DIR) / \"best_hf\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(str(best_dir))\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(str(best_dir))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load test set\n",
    "test_ds, labels = load_test_dataset()\n",
    "\n",
    "# Preprocess test set\n",
    "from transformers import DataCollatorWithPadding\n",
    "collator = DataCollatorWithPadding(tokenizer=feature_extractor, padding=True)\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    out = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = out[\"input_values\"][0]\n",
    "    return batch\n",
    "test_ds_proc = test_ds.map(preprocess)\n",
    "\n",
    "# Run inference and compute metrics\n",
    "all_preds, all_labels = run_inference(model, feature_extractor, device, test_ds_proc)\n",
    "metrics = compute_metrics(all_labels, all_preds, labels)\n",
    "\n",
    "# Save metrics\n",
    "(eval_dir := OUTPUTS_DIR / \"evaluation\").mkdir(parents=True, exist_ok=True)\n",
    "with open(eval_dir / 'metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100b99a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes and Limitations\n",
    "- This notebook is the canonical implementation for the ASR commands (keyword spotting) pipeline in this repository.\n",
    "- All data and model artifacts are stored in the `.cache/` and `outputs/` directories, respectively.\n",
    "- If any code or data step is unclear, incomplete, or fails, please refer to the repository's README or the code comments for further clarification.\n",
    "- No additional scripts or CLI entrypoints exist for this pipeline; all logic is contained in this notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

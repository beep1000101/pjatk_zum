{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13890af",
   "metadata": {},
   "source": [
    "# ASR Commands (Keyword Spotting) â€” Production-grade HF baseline\n",
    "\n",
    "This notebook trains a **keyword spotting / speech command classification** model on TensorFlow's **mini_speech_commands** dataset.\n",
    "\n",
    "## Design goals\n",
    "- Minimal custom modeling code (rely on mature ecosystem).\n",
    "- Deterministic splits + reproducibility.\n",
    "- Clean artifact export (model + processor + labels + metrics).\n",
    "\n",
    "## Architecture\n",
    "1) `data_ingestion/asr_commands` downloads+verifies dataset into `.cache/`\n",
    "2) Dataset is loaded as `{audio, label}` with `datasets.Audio(sampling_rate=16_000)`\n",
    "3) Model is fine-tuned via `transformers.Trainer` using a pretrained speech encoder (`HuBERT` / `Wav2Vec2`)\n",
    "4) Evaluation exports accuracy, macro-F1, and confusion matrix into `outputs/asr_commands/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a404ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio torchcodec -q\n",
    "%pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 torchcodec -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime deps (Colab-friendly). Restart runtime/kernel if you upgrade torch.\n",
    "!pip -q install --upgrade \"transformers>=4.38\" \"datasets[audio]>=2.17\" \"evaluate>=0.4\" \"accelerate>=0.26\" \"scikit-learn>=1.3\" \"soundfile>=0.12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab convenience: clone repo if needed\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"pyproject.toml\").exists():\n",
    "    if not Path(\"pjatk_zum\").exists():\n",
    "        !git clone https://github.com/beep1000101/pjatk_zum.git\n",
    "    else:\n",
    "        print(\"Repo folder already present: pjatk_zum\")\n",
    "else:\n",
    "    print(\"Already in repo root (pyproject.toml found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f90729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab convenience: cd into repo folder if we cloned it\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"pyproject.toml\").exists():\n",
    "    print(\"Already in repo root\")\n",
    "elif Path(\"pjatk_zum\").exists():\n",
    "    %cd pjatk_zum\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find repo root (pyproject.toml) or ./pjatk_zum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest mini_speech_commands into the repo cache\n",
    "!python data_ingestion/asr_commands/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import Audio, Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForAudioClassification,\n",
    "    AutoProcessor,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    " )\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"data_ingestion\").exists() and (p / \"utils\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from utils.paths import CACHE_PATH  # noqa: E402\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "CACHE = CACHE_PATH\n",
    "PIPELINE_CACHE = CACHE / \"asr_commands\"\n",
    "RAW_DIR = PIPELINE_CACHE / \"raw\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\" / \"asr_commands\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9596aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure dataset is present in cache (runs ingestion if needed)\n",
    "mini_root = RAW_DIR / \"mini_speech_commands\"\n",
    "if not mini_root.exists():\n",
    "    print(\"mini_speech_commands not found in cache; running ingestion...\")\n",
    "    !python data_ingestion/asr_commands/run.py\n",
    "\n",
    "assert mini_root.exists(), f\"Missing: {mini_root}\"\n",
    "\n",
    "labels_path = PIPELINE_CACHE / \"labels.json\"\n",
    "if labels_path.exists():\n",
    "    labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "else:\n",
    "    # Fallback: infer from directories\n",
    "    labels = sorted([p.name for p in mini_root.iterdir() if p.is_dir()])\n",
    "\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "labels, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d263554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "down     1000\n",
       "go       1000\n",
       "left     1000\n",
       "no       1000\n",
       "right    1000\n",
       "stop     1000\n",
       "up       1000\n",
       "yes      1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an index of audio files -> labels\n",
    "rows = []\n",
    "for lbl in labels:\n",
    "    for wav_path in sorted((mini_root / lbl).glob(\"*.wav\")):\n",
    "        rows.append({\"audio\": str(wav_path), \"label_str\": lbl, \"label\": int(label2id[lbl])})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"label_str\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6400, 'val': 800, 'test': 800}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deterministic stratified split 80/10/10 using ðŸ¤— Datasets\n",
    "# NOTE: `stratify_by_column` requires a `ClassLabel` feature.\n",
    "from datasets import ClassLabel\n",
    "\n",
    "full_ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "full_ds = full_ds.cast_column(\"label\", ClassLabel(names=labels))\n",
    "\n",
    "# 80% train, 20% temp (val+test)\n",
    "tmp = full_ds.train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"label\")\n",
    "train_ds_raw = tmp[\"train\"]\n",
    "temp_ds = tmp[\"test\"]\n",
    "\n",
    "# Split temp into 50/50 -> 10% val, 10% test\n",
    "tmp2 = temp_ds.train_test_split(test_size=0.5, seed=SEED, stratify_by_column=\"label\")\n",
    "val_ds_raw = tmp2[\"train\"]\n",
    "test_ds_raw = tmp2[\"test\"]\n",
    "\n",
    "ds = DatasetDict({\"train\": train_ds_raw, \"val\": val_ds_raw, \"test\": test_ds_raw})\n",
    "{k: len(v) for k, v in ds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mateusz/dev/pjatk_zum/outputs/asr_commands/preprocessing/splits.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist splits for reuse (stable artifact for later stage-first scripts)\n",
    "(OUTPUTS_DIR / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "splits_path = OUTPUTS_DIR / \"preprocessing\" / \"splits.json\"\n",
    "\n",
    "def _to_records(dset: Dataset) -> list[dict]:\n",
    "    # Keep the same schema used elsewhere in the repo\n",
    "    return [{\"path\": ex[\"audio\"], \"label\": id2label[int(ex[\"label\"]) ]} for ex in dset]\n",
    "\n",
    "payload = {\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"splits\": {\n",
    "        \"train\": _to_records(ds[\"train\"]),\n",
    "        \"val\": _to_records(ds[\"val\"]),\n",
    "        \"test\": _to_records(ds[\"test\"]),\n",
    "    },\n",
    "}\n",
    "splits_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "splits_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a31c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 800, 800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HF audio pipeline: decode/resample -> feature_extractor -> padded batches\n",
    "# Model choice: HuBERT encoder pretrained + KWS-oriented finetuning (SUPERB)\n",
    "MODEL_NAME = \"superb/hubert-base-superb-ks\"\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Decode audio lazily; `datasets` uses soundfile under the hood\n",
    "ds_audio = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    out = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = out[\"input_values\"][0]\n",
    "    if \"attention_mask\" in out:\n",
    "        batch[\"attention_mask\"] = out[\"attention_mask\"][0]\n",
    "    return batch\n",
    "\n",
    "ds_proc = ds_audio.map(preprocess_batch, remove_columns=[\"audio\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(processor=feature_extractor, padding=True)\n",
    "\n",
    "ds_proc[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a877820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m val_ds = SpeechCommandsDataset(examples=val_examples)\n\u001b[32m     52\u001b[39m test_ds = SpeechCommandsDataset(examples=test_examples)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x0, y0 = \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m x0.shape, y0, id_to_label[y0]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mSpeechCommandsDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     37\u001b[39m     ex = \u001b[38;5;28mself\u001b[39m.examples[idx]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     wav, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wav.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m:\n\u001b[32m     40\u001b[39m         wav = wav.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/torchaudio/_torchcodec.py:84\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Parameter validation and warnings\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m normalize:\n",
      "\u001b[31mImportError\u001b[39m: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function."
     ]
    }
   ],
   "source": [
    "# Trainer setup + fine-tuning\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels_np = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels_np)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels_np, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": float(acc), \"f1_macro\": float(f1)}\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Stability on small datasets: freeze encoder features (optional but recommended)\n",
    "if hasattr(model, \"freeze_feature_encoder\"):\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "run_dir = OUTPUTS_DIR / \"hf_training\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(run_dir),\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    " )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_proc[\"train\"],\n",
    "    eval_dataset=ds_proc[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model + processor + labels (portable inference package)\n",
    "best_dir = OUTPUTS_DIR / \"best_hf\"\n",
    "best_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(best_dir))\n",
    "processor.save_pretrained(str(best_dir))\n",
    "(best_dir / \"labels.json\").write_text(json.dumps({\"labels\": labels, \"label2id\": label2id}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "best_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training log history (JSON)\n",
    "train_metrics_path = OUTPUTS_DIR / \"training\" / \"metrics.json\"\n",
    "train_metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history = trainer.state.log_history\n",
    "train_metrics_path.write_text(\n",
    "    json.dumps({\"pipeline\": \"asr_commands\", \"framework\": \"hf_trainer\", \"history\": history}, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "train_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd396e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test split\n",
    "test_metrics = trainer.evaluate(ds_proc[\"test\"])\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + full metrics export\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "pred_out = trainer.predict(ds_proc[\"test\"])\n",
    "logits = pred_out.predictions\n",
    "y_true = pred_out.label_ids\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=list(range(len(labels))),\n",
    "    target_names=labels,\n",
    "    output_dict=True,\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "eval_metrics = {\n",
    "    \"pipeline\": \"asr_commands\",\n",
    "    \"num_test\": int(len(y_true)),\n",
    "    \"accuracy\": float(test_metrics.get(\"eval_accuracy\", test_metrics.get(\"accuracy\", 0.0))),\n",
    "    \"f1_macro\": float(test_metrics.get(\"eval_f1_macro\", test_metrics.get(\"f1_macro\", 0.0))),\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"classification_report\": report,\n",
    "}\n",
    "\n",
    "eval_dir = OUTPUTS_DIR / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_metrics_path = eval_dir / \"metrics.json\"\n",
    "eval_metrics_path.write_text(json.dumps(eval_metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "eval_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Zip outputs for sharing (e.g., Colab -> download)\n",
    "!zip -r asr_commands_outputs.zip outputs/asr_commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e858155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/dev/pjatk_zum/.venv/lib64/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"data_ingestion\").exists() and (p / \"utils\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from utils.paths import CACHE_PATH  # noqa: E402\n",
    "\n",
    "CACHE = CACHE_PATH\n",
    "RAW_DIR = CACHE / \"clip_multimodal\" / \"raw\" / \"cifar-10-batches-py\"\n",
    "OUTPUTS_DIR = ROOT / \"outputs\" / \"clip_multimodal\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b45733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not RAW_DIR.exists():\n",
    "    print(\"CIFAR-10 not found in cache; running ingestion...\")\n",
    "    # This populates: .cache/clip_multimodal/raw/cifar-10-batches-py\n",
    "    # and writes: .cache/clip_multimodal/label_texts.json\n",
    "    !python data_ingestion/clip_multimodal/run.py\n",
    "\n",
    "assert RAW_DIR.exists(), f\"Missing: {RAW_DIR}\"\n",
    "\n",
    "label_texts_path = CACHE / \"clip_multimodal\" / \"label_texts.json\"\n",
    "if label_texts_path.exists():\n",
    "    obj = json.loads(label_texts_path.read_text(encoding=\"utf-8\"))\n",
    "    # In this project we store either a list of strings or a dict like {\"labels\": [...]}\n",
    "    if isinstance(obj, list):\n",
    "        label_list = obj\n",
    "    elif isinstance(obj, dict) and \"labels\" in obj:\n",
    "        label_list = obj[\"labels\"]\n",
    "    elif isinstance(obj, dict) and \"label_texts\" in obj:\n",
    "        label_list = obj[\"label_texts\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label_texts.json format: {type(obj)} keys={list(obj) if isinstance(obj, dict) else None}\")\n",
    "else:\n",
    "    # Fallback (CIFAR-10 class names)\n",
    "    label_list = [\n",
    "        \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "        \"dog\",\"frog\",\"horse\",\"ship\",\"truck\",\n",
    "    ]\n",
    "\n",
    "assert isinstance(label_list, list) and all(isinstance(x, str) for x in label_list)\n",
    "labels = label_list\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "394e5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82519/4006230961.py:4: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  batch = pickle.load(f, encoding=\"bytes\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, (10000, 32, 32, 3), array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CIFAR-10 test split (python batches)\n",
    "test_batch_path = RAW_DIR / \"test_batch\"\n",
    "with open(test_batch_path, \"rb\") as f:\n",
    "    batch = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "# images: (N, 32, 32, 3), uint8\n",
    "images = batch[b\"data\"].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "y_true = np.array(batch[b\"labels\"], dtype=np.int64)\n",
    "\n",
    "len(images), images.shape, y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0e0b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 476.87it/s, Materializing param=visual_projection.weight]                                \n",
      "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "texts = [f\"a photo of a {lbl}\" for lbl in labels]\n",
    "text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_out = model.text_model(\n",
    "        input_ids=text_inputs[\"input_ids\"],\n",
    "        attention_mask=text_inputs.get(\"attention_mask\"),\n",
    "    )\n",
    "    text_features = model.text_projection(text_out.pooler_output)\n",
    "\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d5ecce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/157] 320/10000 images | 12.7 img/s | ETA 12.7 min\n",
      "[10/157] 640/10000 images | 12.1 img/s | ETA 12.9 min\n",
      "[15/157] 960/10000 images | 11.7 img/s | ETA 12.9 min\n",
      "[20/157] 1280/10000 images | 11.7 img/s | ETA 12.4 min\n",
      "[25/157] 1600/10000 images | 11.4 img/s | ETA 12.3 min\n",
      "[30/157] 1920/10000 images | 11.5 img/s | ETA 11.7 min\n",
      "[35/157] 2240/10000 images | 11.5 img/s | ETA 11.2 min\n",
      "[40/157] 2560/10000 images | 11.6 img/s | ETA 10.7 min\n",
      "[45/157] 2880/10000 images | 11.6 img/s | ETA 10.2 min\n",
      "[50/157] 3200/10000 images | 11.6 img/s | ETA 9.8 min\n",
      "[55/157] 3520/10000 images | 11.6 img/s | ETA 9.3 min\n",
      "[60/157] 3840/10000 images | 11.7 img/s | ETA 8.8 min\n",
      "[65/157] 4160/10000 images | 11.7 img/s | ETA 8.3 min\n",
      "[70/157] 4480/10000 images | 11.7 img/s | ETA 7.9 min\n",
      "[75/157] 4800/10000 images | 11.7 img/s | ETA 7.4 min\n",
      "[80/157] 5120/10000 images | 11.6 img/s | ETA 7.0 min\n",
      "[85/157] 5440/10000 images | 11.5 img/s | ETA 6.6 min\n",
      "[90/157] 5760/10000 images | 11.5 img/s | ETA 6.1 min\n",
      "[95/157] 6080/10000 images | 11.5 img/s | ETA 5.7 min\n",
      "[100/157] 6400/10000 images | 11.5 img/s | ETA 5.2 min\n",
      "[105/157] 6720/10000 images | 11.6 img/s | ETA 4.7 min\n",
      "[110/157] 7040/10000 images | 11.6 img/s | ETA 4.3 min\n",
      "[115/157] 7360/10000 images | 11.6 img/s | ETA 3.8 min\n",
      "[120/157] 7680/10000 images | 11.6 img/s | ETA 3.3 min\n",
      "[125/157] 8000/10000 images | 11.6 img/s | ETA 2.9 min\n",
      "[130/157] 8320/10000 images | 11.7 img/s | ETA 2.4 min\n",
      "[135/157] 8640/10000 images | 11.6 img/s | ETA 2.0 min\n",
      "[140/157] 8960/10000 images | 11.5 img/s | ETA 1.5 min\n",
      "[145/157] 9280/10000 images | 11.3 img/s | ETA 1.1 min\n",
      "[150/157] 9600/10000 images | 11.3 img/s | ETA 0.6 min\n",
      "[155/157] 9920/10000 images | 11.2 img/s | ETA 0.1 min\n",
      "[157/157] 10000/10000 images | 11.2 img/s | ETA 0.0 min\n",
      "Done: acc=0.8880 | eval_images=10000 | time=14.87 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N = 10_000  # set 10_000 for full CIFAR-10 test; lower for quick checks\n",
    "batch_size = 64\n",
    "log_every = 5  # batches\n",
    "\n",
    "n = min(N, len(images))\n",
    "preds: list[int] = []\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "last_log_t = t0\n",
    "n_batches = (n + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_i, start in enumerate(range(0, n, batch_size), start=1):\n",
    "    end = min(start + batch_size, n)\n",
    "    batch_imgs = [Image.fromarray(images[i]) for i in range(start, end)]\n",
    "\n",
    "    img_inputs = processor(images=batch_imgs, return_tensors=\"pt\")\n",
    "    pixel_values = img_inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_out = model.vision_model(pixel_values=pixel_values)\n",
    "        img_features = model.visual_projection(vision_out.pooler_output)\n",
    "\n",
    "    img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "    logits = img_features @ text_features.T\n",
    "    preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "\n",
    "    if batch_i % log_every == 0 or end == n:\n",
    "        now = time.perf_counter()\n",
    "        elapsed = now - t0\n",
    "        done = end\n",
    "        rate = done / elapsed if elapsed > 0 else float(\"inf\")\n",
    "        remaining = (n - done) / rate if rate > 0 else float(\"inf\")\n",
    "        print(f\"[{batch_i}/{n_batches}] {done}/{n} images | {rate:.1f} img/s | ETA {remaining/60:.1f} min\")\n",
    "        last_log_t = now\n",
    "\n",
    "y_pred = np.array(preds, dtype=np.int64)\n",
    "acc = float((y_pred == y_true[:n]).mean())\n",
    "total_s = time.perf_counter() - t0\n",
    "print(f\"Done: acc={acc:.4f} | eval_images={n} | time={total_s/60:.2f} min\")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3fdd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/mateusz/dev/pjatk_zum/outputs/clip_multimodal/evaluation/metrics.json'),\n",
       " PosixPath('/home/mateusz/dev/pjatk_zum/outputs/clip_multimodal/evaluation/sample_predictions.csv'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"pipeline\": \"clip_multimodal\",\n",
    "    \"model_id\": model_id,\n",
    "    \"num_eval\": int(n),\n",
    "    \"top1_accuracy\": acc,\n",
    "    \"labels\": labels,\n",
    "    \"prompt_template\": \"a photo of a {label}\",\n",
    "}\n",
    "\n",
    "(OUTPUTS_DIR / \"evaluation\").mkdir(parents=True, exist_ok=True)\n",
    "metrics_path = OUTPUTS_DIR / \"evaluation\" / \"metrics.json\"\n",
    "metrics_path.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "rows = []\n",
    "for i in range(min(50, n)):\n",
    "    rows.append({\n",
    "        \"idx\": i,\n",
    "        \"true_id\": int(y_true[i]),\n",
    "        \"true_label\": labels[int(y_true[i])],\n",
    "        \"pred_id\": int(y_pred[i]),\n",
    "        \"pred_label\": labels[int(y_pred[i])],\n",
    "    })\n",
    "\n",
    "report_df = pd.DataFrame(rows)\n",
    "report_csv = OUTPUTS_DIR / \"evaluation\" / \"sample_predictions.csv\"\n",
    "report_df.to_csv(report_csv, index=False)\n",
    "\n",
    "metrics_path, report_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Zip outputs for sharing (e.g., Colab -> download)\n",
    "!zip -r clip_multimodal_outputs.zip outputs/clip_multimodal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

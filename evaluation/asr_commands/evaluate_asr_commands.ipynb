{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20524903",
   "metadata": {},
   "source": [
    "# ASR Commands Model Evaluation\n",
    "\n",
    "This notebook loads the archived ASR commands model and evaluates it on the test set. Metrics and visualizations are saved to the evaluation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "from datasets import Dataset, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EVAL_DIR = Path('../evaluation/asr_commands').resolve()\n",
    "MODEL_DIR = Path('../../models/asr_commands_best').resolve()  # Adjust if needed\n",
    "SPLITS_PATH = Path('../../outputs/asr_commands/preprocessing/splits.json').resolve()\n",
    "\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "from eval_utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test split and labels\n",
    "with open(SPLITS_PATH, 'r', encoding='utf-8') as f:\n",
    "    splits = json.load(f)\n",
    "labels = splits['labels']\n",
    "test_records = splits['splits']['test']\n",
    "\n",
    "test_df = [\n",
    "    {'audio': r['path'], 'label': labels.index(r['label'])} for r in test_records\n",
    "]\n",
    "test_ds = Dataset.from_list(test_df).cast_column('audio', Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and feature extractor\n",
    "model = AutoModelForAudioClassification.from_pretrained(MODEL_DIR)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d103b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test set\n",
    "def preprocess(batch):\n",
    "    audio = batch['audio']\n",
    "    out = feature_extractor(audio['array'], sampling_rate=audio['sampling_rate'])\n",
    "    batch['input_values'] = out['input_values'][0]\n",
    "    return batch\n",
    "\n",
    "test_ds_proc = test_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for ex in test_ds_proc:\n",
    "        inp = torch.tensor(ex['input_values']).unsqueeze(0).to(device)\n",
    "        logits = model(inp).logits.cpu().numpy()[0]\n",
    "        pred = np.argmax(logits)\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(ex['label'])\n",
    "all_preds, all_labels = np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save metrics\n",
    "metrics = compute_metrics(all_labels, all_preds, labels)\n",
    "with open(EVAL_DIR / 'metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / 'confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff69adf",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Metrics are saved to `metrics.json` in the evaluation directory.\n",
    "- Confusion matrix is saved as `confusion_matrix.png`.\n",
    "- See the classification report in the metrics output above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

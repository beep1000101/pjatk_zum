{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40742048",
   "metadata": {},
   "source": [
    "# CLIP Multimodal Model Evaluation\n",
    "\n",
    "This notebook loads a pretrained CLIP model and evaluates it on the test set. Metrics and visualizations are saved to the evaluation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554165d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EVAL_DIR = Path('../evaluation/clip_multimodal').resolve()\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch16'  # or your custom path if fine-tuned\n",
    "SPLITS_PATH = Path('../../outputs/clip_multimodal/preprocessing/splits.json').resolve()\n",
    "\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "from eval_utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test split and labels\n",
    "with open(SPLITS_PATH, 'r', encoding='utf-8') as f:\n",
    "    splits = json.load(f)\n",
    "labels = splits['labels']\n",
    "test_records = splits['splits']['test']\n",
    "\n",
    "test_df = pd.DataFrame([{'image': r['image'], 'text': r['text'], 'label': labels.index(r['label'])} for r in test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for _, row in test_df.iterrows():\n",
    "        inputs = processor(text=row['text'], images=row['image'], return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image.cpu().numpy()[0]\n",
    "        pred = np.argmax(logits_per_image)\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(row['label'])\n",
    "all_preds, all_labels = np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e292ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save metrics\n",
    "metrics = compute_metrics(all_labels, all_preds, labels)\n",
    "with open(EVAL_DIR / 'metrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig(EVAL_DIR / 'confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf425fe1",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Metrics are saved to `metrics.json` in the evaluation directory.\n",
    "- Confusion matrix is saved as `confusion_matrix.png`.\n",
    "- See the classification report in the metrics output above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
